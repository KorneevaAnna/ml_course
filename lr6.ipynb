{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №6\n",
    "## Корнеева Анна, ИУ5-23М\n",
    "**Тема: Классификация текста.**\n",
    "\n",
    "**Задание:**\n",
    "\n",
    "Для произвольного набора данных, предназначенного для классификации текстов, решите задачу классификации текста двумя способами:\n",
    "\n",
    "Способ 1. На основе CountVectorizer или TfidfVectorizer.\n",
    "\n",
    "Способ 2. На основе моделей word2vec или Glove или fastText.\n",
    "\n",
    "Сравните качество полученных моделей.\n",
    "Для поиска наборов данных в поисковой системе можно использовать ключевые слова \"datasets for text classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:43.687948Z",
     "iopub.status.busy": "2021-06-08T18:50:43.687318Z",
     "iopub.status.idle": "2021-06-08T18:50:45.514661Z",
     "shell.execute_reply": "2021-06-08T18:50:45.513670Z",
     "shell.execute_reply.started": "2021-06-08T18:50:43.687819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\n",
      "/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:45.516564Z",
     "iopub.status.busy": "2021-06-08T18:50:45.516244Z",
     "iopub.status.idle": "2021-06-08T18:50:45.525667Z",
     "shell.execute_reply": "2021-06-08T18:50:45.524632Z",
     "shell.execute_reply.started": "2021-06-08T18:50:45.516533Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:45.527551Z",
     "iopub.status.busy": "2021-06-08T18:50:45.527235Z",
     "iopub.status.idle": "2021-06-08T18:50:45.931897Z",
     "shell.execute_reply": "2021-06-08T18:50:45.931028Z",
     "shell.execute_reply.started": "2021-06-08T18:50:45.527521Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv')\n",
    "test = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:45.933909Z",
     "iopub.status.busy": "2021-06-08T18:50:45.933506Z",
     "iopub.status.idle": "2021-06-08T18:50:45.939101Z",
     "shell.execute_reply": "2021-06-08T18:50:45.938239Z",
     "shell.execute_reply.started": "2021-06-08T18:50:45.933864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41157, 6)\n",
      "(3798, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:45.940497Z",
     "iopub.status.busy": "2021-06-08T18:50:45.940178Z",
     "iopub.status.idle": "2021-06-08T18:50:45.974071Z",
     "shell.execute_reply": "2021-06-08T18:50:45.973185Z",
     "shell.execute_reply.started": "2021-06-08T18:50:45.940468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:45.975498Z",
     "iopub.status.busy": "2021-06-08T18:50:45.975216Z",
     "iopub.status.idle": "2021-06-08T18:50:45.994554Z",
     "shell.execute_reply": "2021-06-08T18:50:45.993503Z",
     "shell.execute_reply.started": "2021-06-08T18:50:45.975463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:48.916584Z",
     "iopub.status.busy": "2021-06-08T18:50:48.916188Z",
     "iopub.status.idle": "2021-06-08T18:50:48.954568Z",
     "shell.execute_reply": "2021-06-08T18:50:48.953641Z",
     "shell.execute_reply.started": "2021-06-08T18:50:48.916548Z"
    }
   },
   "outputs": [],
   "source": [
    "train.Sentiment = train.Sentiment.replace({'Extremely Positive':'Positive','Extremely Negative':'Negative'})\n",
    "test.Sentiment = test.Sentiment.replace({'Extremely Positive':'Positive','Extremely Negative':'Negative'})\n",
    "\n",
    "lenc = LabelEncoder()\n",
    "test.Sentiment = lenc.fit_transform(test.Sentiment)\n",
    "train.Sentiment = lenc.fit_transform(train.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:49.002362Z",
     "iopub.status.busy": "2021-06-08T18:50:49.001990Z",
     "iopub.status.idle": "2021-06-08T18:50:49.014527Z",
     "shell.execute_reply": "2021-06-08T18:50:49.013366Z",
     "shell.execute_reply.started": "2021-06-08T18:50:49.002329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet  Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          1  \n",
       "1  advice Talk to your neighbours family to excha...          2  \n",
       "2  Coronavirus Australia: Woolworths to give elde...          2  \n",
       "3  My food stock is not the only one which is emp...          2  \n",
       "4  Me, ready to go at supermarket during the #COV...          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:49.660207Z",
     "iopub.status.busy": "2021-06-08T18:50:49.659855Z",
     "iopub.status.idle": "2021-06-08T18:50:49.665556Z",
     "shell.execute_reply": "2021-06-08T18:50:49.664386Z",
     "shell.execute_reply.started": "2021-06-08T18:50:49.660173Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = train['OriginalTweet']\n",
    "y_train = train['Sentiment']\n",
    "x_test = test['OriginalTweet']\n",
    "y_test = test['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Очистка данных\n",
    "\n",
    "1. приведение всех слов к нижнему регистру\n",
    "2. удаление ссылок\n",
    "3. отделение слов и знаков пунктуации пробелом\n",
    "4. удаление все кроме (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "5. тоенизация - разделение строки на слова, для удаления стоп-слов\n",
    "6. удалние стоп-слов, т.е. часто используемых слов не несущих большой смысловой нагрузки\n",
    "7. создание строки из токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:50.382400Z",
     "iopub.status.busy": "2021-06-08T18:50:50.382030Z",
     "iopub.status.idle": "2021-06-08T18:50:50.389979Z",
     "shell.execute_reply": "2021-06-08T18:50:50.388668Z",
     "shell.execute_reply.started": "2021-06-08T18:50:50.382360Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "    w = w.lower()\n",
    "    w = re.sub('\\t\\n', '', w)\n",
    "    w = re.sub(r'http\\S+', '', w)\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,`']+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "    tokens = w.split(' ')\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) # remove stopwords\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:50:51.083115Z",
     "iopub.status.busy": "2021-06-08T18:50:51.082766Z",
     "iopub.status.idle": "2021-06-08T18:51:00.448961Z",
     "shell.execute_reply": "2021-06-08T18:51:00.448025Z",
     "shell.execute_reply.started": "2021-06-08T18:50:51.083084Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.apply(preprocess_sentence)\n",
    "x_test = x_test.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:00.450318Z",
     "iopub.status.busy": "2021-06-08T18:51:00.450042Z",
     "iopub.status.idle": "2021-06-08T18:51:00.459206Z",
     "shell.execute_reply": "2021-06-08T18:51:00.458331Z",
     "shell.execute_reply.started": "2021-06-08T18:51:00.450292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['advice talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist gp set online shopping accounts poss adequate supplies regular meds order',\n",
       " 'coronavirus australia woolworths give elderly , disabled dedicated shopping hours amid covid outbreak',\n",
       " 'food stock one empty . . . please , panic , enough food everyone take need . stay calm , stay safe . covid france covid covid coronavirus confinement confinementotal confinementgeneral',\n",
       " \", ready go supermarket covid outbreak . i'm paranoid , food stock litteraly empty . coronavirus serious thing , please , panic . causes shortage . . . coronavirusfrance restezchezvous stayathome confinement\",\n",
       " 'news region first confirmed covid case came sullivan county last week , people flocked area stores purchase cleaning supplies , hand sanitizer , food , toilet paper goods , tim dodson reports',\n",
       " \"cashier grocery store sharing insights covid prove credibility commented i'm civics class know i'm talking .\",\n",
       " 'supermarket today . buy toilet paper . rebel toiletpapercrisis covid',\n",
       " 'due covid retail store classroom atlanta open walk business classes next two weeks , beginning monday , march . continue process online phone orders normal ! thank understanding !',\n",
       " 'corona prevention , stop buy things cash use online payment methods corona spread notes . also prefer online shopping home . time fight covid ? . govindia indiafightscorona']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сформируем общий словарь для обучения моделей из обучающей и тестовой выборки\n",
    "vocab_list = x_train.tolist() + x_test.tolist()\n",
    "print(len(vocab_list))\n",
    "vocab_list[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:05.115512Z",
     "iopub.status.busy": "2021-06-08T18:51:05.115152Z",
     "iopub.status.idle": "2021-06-08T18:51:05.121542Z",
     "shell.execute_reply": "2021-06-08T18:51:05.120730Z",
     "shell.execute_reply.started": "2021-06-08T18:51:05.115481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['OriginalTweet'][0:10][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:05.574546Z",
     "iopub.status.busy": "2021-06-08T18:51:05.573933Z",
     "iopub.status.idle": "2021-06-08T18:51:06.949957Z",
     "shell.execute_reply": "2021-06-08T18:51:06.948982Z",
     "shell.execute_reply.started": "2021-06-08T18:51:05.574501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 54546\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T17:49:48.968380Z",
     "iopub.status.busy": "2021-06-08T17:49:48.968091Z",
     "iopub.status.idle": "2021-06-08T17:49:48.975761Z",
     "shell.execute_reply": "2021-06-08T17:49:48.974835Z",
     "shell.execute_reply.started": "2021-06-08T17:49:48.968354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phil=36274\n",
      "gahan=18888\n",
      "chrisitv=8219\n",
      "advice=670\n",
      "talk=47397\n",
      "neighbours=32568\n",
      "family=16737\n",
      "exchange=16272\n",
      "phone=36318\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T17:49:48.978125Z",
     "iopub.status.busy": "2021-06-08T17:49:48.977697Z",
     "iopub.status.idle": "2021-06-08T17:49:57.394080Z",
     "shell.execute_reply": "2021-06-08T17:49:57.393096Z",
     "shell.execute_reply.started": "2021-06-08T17:49:48.978083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<44955x1141506 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2263622 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_ngram_features = tfidfv.fit_transform(vocab_list)\n",
    "tfidf_ngram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:55:18.411789Z",
     "iopub.status.busy": "2021-06-08T18:55:18.411211Z",
     "iopub.status.idle": "2021-06-08T18:55:18.418165Z",
     "shell.execute_reply": "2021-06-08T18:55:18.417318Z",
     "shell.execute_reply.started": "2021-06-08T18:55:18.411735Z"
    }
   },
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, x_train[:10000], y_train[:10000], scoring='accuracy', cv=3,).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T17:49:57.405270Z",
     "iopub.status.busy": "2021-06-08T17:49:57.404982Z",
     "iopub.status.idle": "2021-06-08T17:53:35.413804Z",
     "shell.execute_reply": "2021-06-08T17:53:35.412780Z",
     "shell.execute_reply.started": "2021-06-08T17:49:57.405243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - RandomForestClassifier()\n",
      "Accuracy = 0.6910992838935964\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - ComplementNB()\n",
      "Accuracy = 0.6462999629297063\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, max_iter=1000)\n",
      "Accuracy = 0.7155993643755497\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.7094994142485634\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - RandomForestClassifier()\n",
      "Accuracy = 0.6725986035916129\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - ComplementNB()\n",
      "Accuracy = 0.6464996829616975\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, max_iter=1000)\n",
      "Accuracy = 0.7047998441115858\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.7113996042675653\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [RandomForestClassifier(), ComplementNB(), LogisticRegression(C=3.0, solver='lbfgs', max_iter=1000), LinearSVC()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лучший результат покаазала модель LogisticRegression(C=3.0, max_iter=1000) с CountVectorizer\n",
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:17.044417Z",
     "iopub.status.busy": "2021-06-08T18:51:17.044046Z",
     "iopub.status.idle": "2021-06-08T18:51:17.341561Z",
     "shell.execute_reply": "2021-06-08T18:51:17.340826Z",
     "shell.execute_reply.started": "2021-06-08T18:51:17.044381Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:17.343071Z",
     "iopub.status.busy": "2021-06-08T18:51:17.342692Z",
     "iopub.status.idle": "2021-06-08T18:51:20.116973Z",
     "shell.execute_reply": "2021-06-08T18:51:20.116197Z",
     "shell.execute_reply.started": "2021-06-08T18:51:17.343043Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in vocab_list:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:20.118802Z",
     "iopub.status.busy": "2021-06-08T18:51:20.118391Z",
     "iopub.status.idle": "2021-06-08T18:51:20.123890Z",
     "shell.execute_reply": "2021-06-08T18:51:20.123167Z",
     "shell.execute_reply.started": "2021-06-08T18:51:20.118771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advice',\n",
       " 'talk',\n",
       " 'neighbours',\n",
       " 'family',\n",
       " 'exchange',\n",
       " 'phone',\n",
       " 'numbers',\n",
       " 'create',\n",
       " 'contact',\n",
       " 'list',\n",
       " 'phone',\n",
       " 'numbers',\n",
       " 'neighbours',\n",
       " 'schools',\n",
       " 'employer',\n",
       " 'chemist',\n",
       " 'gp',\n",
       " 'set',\n",
       " 'online',\n",
       " 'shopping',\n",
       " 'accounts',\n",
       " 'poss',\n",
       " 'adequate',\n",
       " 'supplies',\n",
       " 'regular',\n",
       " 'meds',\n",
       " 'order']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:20.125569Z",
     "iopub.status.busy": "2021-06-08T18:51:20.125176Z",
     "iopub.status.idle": "2021-06-08T18:51:24.581597Z",
     "shell.execute_reply": "2021-06-08T18:51:24.580701Z",
     "shell.execute_reply.started": "2021-06-08T18:51:20.125539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 80.1 ms, total: 12.4 s\n",
      "Wall time: 4.44 s\n"
     ]
    }
   ],
   "source": [
    "%time model = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:51:24.583040Z",
     "iopub.status.busy": "2021-06-08T18:51:24.582769Z",
     "iopub.status.idle": "2021-06-08T18:51:24.595839Z",
     "shell.execute_reply": "2021-06-08T18:51:24.594530Z",
     "shell.execute_reply.started": "2021-06-08T18:51:24.583013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('looking', 0.8133401274681091), ('try', 0.7777470946311951), ('gift', 0.7415961027145386), ('easy', 0.7386958599090576), ('meal', 0.7276292443275452)]\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что модель обучилась\n",
    "print(model.wv.most_similar(positive=['find'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:55:22.970237Z",
     "iopub.status.busy": "2021-06-08T18:55:22.969890Z",
     "iopub.status.idle": "2021-06-08T18:59:05.247827Z",
     "shell.execute_reply": "2021-06-08T18:59:05.246891Z",
     "shell.execute_reply.started": "2021-06-08T18:55:22.970205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - RandomForestClassifier()\n",
      "Accuracy = 0.6933987639915761\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - ComplementNB()\n",
      "Accuracy = 0.6462999629297063\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, max_iter=1000)\n",
      "Accuracy = 0.7155993643755497\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.7094994142485634\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - RandomForestClassifier()\n",
      "Accuracy = 0.6774992436256223\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - ComplementNB()\n",
      "Accuracy = 0.6464996829616975\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, max_iter=1000)\n",
      "Accuracy = 0.7047998441115858\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'aa': 0, 'aaa': 1, 'aaaaakubosan': 2, 'aaaaas': 3,\n",
      "                            'aaaand': 4, 'aaachatterjee': 5, 'aaanews': 6,\n",
      "                            'aaannnddd': 7, 'aaanortheast': 8, 'aabutan': 9,\n",
      "                            'aacopd': 10, 'aacounty': 11, 'aacountygovt': 12,\n",
      "                            'aadeshrawal': 13, 'aadya': 14, 'aadyasitara': 15,\n",
      "                            'aafp': 16, 'aahealth': 17, 'aahh': 18, 'aai': 19,\n",
      "                            'aaisp': 20, 'aajeevika': 21, 'aajtak': 22,\n",
      "                            'aakash': 23, 'aalonzowatt': 24, 'aalto': 25,\n",
      "                            'aaltouniversity': 26, 'aalwajih': 27,\n",
      "                            'aamaadmi': 28, 'aamaadmiparty': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.7113996042675653\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [RandomForestClassifier(), ComplementNB(), LogisticRegression(C=3.0, solver='lbfgs', max_iter=1000), LinearSVC()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:59:05.250942Z",
     "iopub.status.busy": "2021-06-08T18:59:05.250609Z",
     "iopub.status.idle": "2021-06-08T18:59:05.256726Z",
     "shell.execute_reply": "2021-06-08T18:59:05.255741Z",
     "shell.execute_reply.started": "2021-06-08T18:59:05.250909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44955"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T18:59:05.259288Z",
     "iopub.status.busy": "2021-06-08T18:59:05.258868Z",
     "iopub.status.idle": "2021-06-08T18:59:05.288756Z",
     "shell.execute_reply": "2021-06-08T18:59:05.287402Z",
     "shell.execute_reply.started": "2021-06-08T18:59:05.259228Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bc32498b17da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "x_train.value.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-08T18:59:05.289781Z",
     "iopub.status.idle": "2021-06-08T18:59:05.290183Z"
    }
   },
   "outputs": [],
   "source": [
    "boundary = 30000\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:boundary+1000]\n",
    "Y_train = y_train[:boundary]\n",
    "Y_test = y_train[boundary:boundary+1000]\n",
    "\n",
    "def sentiment(v, c):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            model = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            model.fit(X_train, Y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            \n",
    "            print_accuracy_score_for_classes(Y_test, y_pred)\n",
    "\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-08T18:59:05.291317Z",
     "iopub.status.idle": "2021-06-08T18:59:05.291743Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-08T18:59:05.292612Z",
     "iopub.status.idle": "2021-06-08T18:59:05.292996Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizers_list = [EmbeddingVectorizer(model.wv)]\n",
    "classifiers_list = [RandomForestClassifier(),  LogisticRegression(C=3.0, solver='lbfgs', max_iter=1000), LinearSVC()]\n",
    "sentiment(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
